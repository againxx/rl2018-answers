# Exercises of  Chapter 05

## 5.5
#### Ex5.5 考虑只有一个非终止状态和一个单一动作的马尔科夫决策过程, 动作设定为: 以概率$p$跳回非终止状态, 以概率$1-p$转移到终止状态. 令每一时刻所有转移的收益都为$+1$, 且$\gamma=1$. 假设你观察到一幕序列持续了10个时刻, 获得了值为10的回报. 则对这个非终止状态的首次访问型和每次访问型的价值估计分别是什么?
解: 对于首次访问型, 仅仅考虑$S_0$时刻的估计, 即$V(S_0) = G_0 = 10$

对于每次访问型, 需要考虑除最后时刻外的所有时刻的估计
$$ V(S) = \frac{1}{10} (G_0 + G_1 + \cdots + G_9) = \frac{10 + 9 + \cdots + 1}{10} = 5.5 $$

#### Ex5.6 给定用$b$生成的回报, 类比公式(5.6), 用动作价值函数$Q(s, a)$替代状态价值函数$V(s)$, 得到的式子是什么?
解:
$$ \mathbb{E}_b [\rho_{t+1:T-1} G_t | S_t = s, A_t = a] = q_\pi(s, a) $$

故加权重要度采样估计的$Q(s, a)$为
$$
Q(s, a) = \frac{\sum_{t \in {\large\tau}(s, a)}\rho_{t+1:T(t)-1} G_t}{\sum_{t \in {\large\tau}(s, a)}\rho_{t+1:T(t)-1}}
$$
其中${\large\tau}(s,a)$为所有访问过状态$s$并选择动作$a$的时刻的集合

#### Ex5.7 实际在使用普通重要度采样时, 与图5.3所示的学习曲线一样, 错误率随着训练一般都是下降的. 但是对于加权重要度采样, 错误率会先上升然后下降. 为什么会这样?
解: 在刚开始的时候, 由于behavior策略是随机的, 很少会达到20点以上, 而target策略是到达20点以上才停牌.
所以对应的重要度采样比都为0, 从而估计的状态一直为0, 恰好和正确估计值比较接近. 随着experience不断积累, 误差开始显现, 随后随着训练又再次下降.

#### Ex5.8 在图5.4和例5.5的结果中采用了首次访问型MC方法. 假设在同样的问题中采用了每次访问型MC方法. 估计器的方差仍会是无穷吗? 为什么?
解: 仍是无穷的, 可以验证均值依然有界(放大了不到两倍), 但是方差求和式中会增加一些项, 而且增加的项均为正, 所以总求和依然为无穷.

## 5.6
#### Ex5.9 对使用首次访问型蒙特卡洛的策略评估 (5.1节) 过程进行修改, 要求采用2.4节中描述的对样本平均的增量式实现.
解:

对所有$s \in S$, 初始化$C(s) = 0$

对本幕中的每一步进行循环, $t = T-1, T-2, \cdots, 0:$

* $G \leftarrow \gamma G + R_{t+1}$

* 除非$S_t$在$S_0, S_1, \cdots, S_{t-1}$中已出现过:
    - $C(S_t) \leftarrow C(S_t) + 1$
    - $V(S_t) \leftarrow V(S_t) + \frac{1}{C(S_t)} [G - V(S_t)]$

#### Ex5.10 从式(5.7)推导出加权平均的更新规则(式5.8). 推导时遵循非加权规则(式2.3)的推导思路.
解:
$$
\begin{aligned}
    V_{n+1} &= \frac{\sum_{k=1}^n W_k G_k}{\sum_{k=1}^n W_k} \\
            &= \frac{1}{\sum_{k=1}^n W_k}(W_n G_n + \sum_{k=1}^{n-1} W_k G_k) \\
            &= \frac{1}{\sum_{k=1}^n W_k}(W_n G_n + (\sum_{k=1}^{n-1} W_k)\frac{\sum_{k=1}^{n-1} W_k G_k}{\sum_{k=1}^{n-1} W_k}) \\
            &= \frac{1}{\sum_{k=1}^n W_k}(W_n G_n + (\sum_{k=1}^{n-1} W_k)V_n) \\
            &= \frac{1}{\sum_{k=1}^n W_k}(W_n G_n + (C_n - W_n)V_n) \\
            &= V_n + \frac{W_n}{C_n}(G_n - V_n)
\end{aligned}
$$
